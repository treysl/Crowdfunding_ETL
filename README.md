# Crowdfunding ETL

For the ETL mini project, I worked with a partner to practice building an ETL pipeline using Python, Pandas, and Python dictionary methods and regular expressions to extract and transform the data. After we transformed the data, we created four CSV files and use the CSV file data to create an ERD and a table schema. Finally, we uploaded the CSV file data into a Postgres database.

## Four CSV file - category, subcategory, contacts, and campaign exported as CSV file in the "RESOURCES" folder.

![image](https://user-images.githubusercontent.com/119129801/228131072-7c68cc8f-d968-4a18-a303-7520d8aafb7c.png)


![image](https://user-images.githubusercontent.com/119129801/228131096-ec26d443-d832-4928-8a89-54a1f2d0b080.png)


![image](https://user-images.githubusercontent.com/119129801/228131197-c55bcafd-21d5-4447-8811-8e567480fed3.png)


![image](https://user-images.githubusercontent.com/119129801/228131250-0e880299-5924-4859-a58d-11c4d4bc00d5.png)


## The four CSV file data are used to create an ERD and a table schema.

![image](https://user-images.githubusercontent.com/119129801/228130360-e3b46f7c-bb6f-493b-9d52-963e7bfcaba8.png)

The information from the ERD is used to create a table schema for each CSV file. Each of the four CSV file are imported into its corresponding SQL table. The table created is verified that each table has the correct data by running a SELECT statement for each.
